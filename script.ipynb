{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6047f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_skills = pd.read_csv('D:\\Coding Playground\\WebDev\\projects\\job matcher\\job_skills.csv')\n",
    "linkedin_job_posting = pd.read_csv('D:\\Coding Playground\\WebDev\\projects\\job matcher\\linkedin_job_postings.csv')\n",
    "\n",
    "linkedin_job_posting = pd.merge(linkedin_job_posting, job_skills, on='job_link', how = 'inner')\n",
    "\n",
    "columns_to_drop = [\n",
    "    'last_processed_time', 'got_summary', 'got_ner', 'is_being_worked',\n",
    "    'job_location', 'first_seen', 'search_city', 'search_country'\n",
    "]\n",
    "linkedin_job_posting = linkedin_job_posting.drop(columns=columns_to_drop)\n",
    "\n",
    "# Replace 'job_link' with a unique numeric ID\n",
    "linkedin_job_posting.reset_index(drop=True, inplace=True)\n",
    "linkedin_job_posting.insert(0, 'job_id', linkedin_job_posting.index)\n",
    "\n",
    "# Optionally drop 'job_link' if you no longer need it\n",
    "linkedin_job_posting = linkedin_job_posting.drop(columns=['job_link'])\n",
    "\n",
    "# Remove rows where job_title or job_skills is NaN or empty string\n",
    "linkedin_job_posting = linkedin_job_posting[\n",
    "    linkedin_job_posting['job_title'].notna() &\n",
    "    linkedin_job_posting['job_title'].astype(str).str.strip().ne('') &\n",
    "    linkedin_job_posting['job_skills'].notna() &\n",
    "    linkedin_job_posting['job_skills'].astype(str).str.strip().ne('')\n",
    "]\n",
    "\n",
    "linkedin_job_posting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Load and extract job skills\n",
    "df = linkedin_job_posting[[\"job_title\", \"job_skills\"]].dropna()\n",
    "\n",
    "# STEP 2: Clean each skill\n",
    "def clean_skill(skill):\n",
    "    skill = skill.lower().strip()\n",
    "    skill = re.sub(r\"[^a-z0-9+.#\\- ]\", \"\", skill)  # retain common technical symbols\n",
    "    skill = re.sub(r\"\\s+\", \" \", skill)\n",
    "    return skill\n",
    "\n",
    "def clean_and_split_skills(skill_string):\n",
    "    return [clean_skill(s) for s in str(skill_string).split(\",\") if s.strip()]\n",
    "\n",
    "df[\"job_skill_list\"] = df[\"job_skills\"].progress_apply(clean_and_split_skills)\n",
    "\n",
    "# STEP 3: Extract all unique cleaned skills\n",
    "print(\"Extracting unique skills...\")\n",
    "all_skills = set()\n",
    "df[\"job_skill_list\"].apply(lambda skills: all_skills.update(skills))\n",
    "\n",
    "# Optional: filter out junk skills\n",
    "print(\"Filtering noisy skills...\")\n",
    "all_skills = {\n",
    "    s for s in all_skills\n",
    "    if 2 <= len(s) <= 50 and not any(keyword in s for keyword in [\"looking for\", \"please\", \"experience in\"])\n",
    "}\n",
    "\n",
    "print(f\"Cleaned unique skills count: {len(all_skills):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa059587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Encoding skill vocabulary...\")\n",
    "skill_list = sorted(all_skills)\n",
    "skill_embeddings = model.encode(skill_list, batch_size=128, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_embeddings = np.load('skill_embeddings.npy')\n",
    "skill_list = np.load('skill_list.npy')\n",
    "skill_to_embedding = dict(zip(skill_list, skill_embeddings))\n",
    "\n",
    "with open(\"skill_to_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(skill_to_embedding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_skills_fast(skill_list_raw, threshold=0.8):\n",
    "    skill_list = list(set([clean_skill(s) for s in skill_list_raw if s.strip()]))\n",
    "    if not skill_list:\n",
    "        return []\n",
    "\n",
    "    clustered = []\n",
    "    used = set()\n",
    "    for i, skill_i in enumerate(skill_list):\n",
    "        if i in used or skill_i not in skill_to_embedding:\n",
    "            continue\n",
    "        group = [skill_i]\n",
    "        used.add(i)\n",
    "        for j in range(i + 1, len(skill_list)):\n",
    "            if j in used:\n",
    "                continue\n",
    "            skill_j = skill_list[j]\n",
    "            if skill_j not in skill_to_embedding:\n",
    "                continue\n",
    "            sim = util.cos_sim(skill_to_embedding[skill_i], skill_to_embedding[skill_j])\n",
    "            if sim.item() >= threshold:\n",
    "                group.append(skill_j)\n",
    "                used.add(j)\n",
    "        clustered.append(min(group, key=len))\n",
    "    return list(set(clustered))\n",
    "\n",
    "# Apply to each row\n",
    "print(\"Normalizing skills for each job...\")\n",
    "df[\"normalized_skills\"] = df[\"job_skill_list\"].progress_apply(lambda x: normalize_skills_fast(x))\n",
    "\n",
    "# Create text representation\n",
    "df[\"skill_text\"] = df[\"normalized_skills\"].apply(lambda skills: \" \".join(skills))\n",
    "\n",
    "df.to_csv(\"jobs_normalized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf96b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"job_text\"] = df[\"job_title\"].fillna(\"\") + \" - \" + df[\"skill_text\"].fillna(\"\")\n",
    "\n",
    "print(\"Encoding job postings...\")\n",
    "job_embeddings = model.encode(df[\"job_text\"].tolist(), show_progress_bar=True, batch_size=128)\n",
    "\n",
    "np.save(\"job_embeddings.npy\", job_embeddings)\n",
    "df.to_csv(\"jobs_with_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(384)\n",
    "index.add(job_embeddings)\n",
    "\n",
    "faiss.write_index(index, \"faiss_jobs.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05366cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse(skill_str):\n",
    "    if isinstance(skill_str, str):\n",
    "        return [s.strip().strip(\"'\\\"\") for s in skill_str.strip(\"[]\").split(\",\") if s.strip()]\n",
    "    return []\n",
    "\n",
    "df[\"normalized_skills\"] = df[\"normalized_skills\"].progress_apply(safe_parse)\n",
    "\n",
    "# Create a job key to group similar job titles together\n",
    "df[\"job_key\"] = df[\"job_title\"].str.lower().str.strip()\n",
    "\n",
    "# Sort by job title to group duplicates close together\n",
    "df = df.sort_values(\"job_key\").reset_index(drop=True)\n",
    "\n",
    "# Track rows to keep\n",
    "keep_indices = []\n",
    "\n",
    "seen = set()\n",
    "\n",
    "# Iterate through rows\n",
    "for i in tqdm(range(len(df))):\n",
    "    if i in seen:\n",
    "        continue\n",
    "\n",
    "    current_title = df.loc[i, \"job_key\"]\n",
    "    current_skills = set(df.loc[i, \"normalized_skills\"])\n",
    "    current_idx = i\n",
    "    keep_indices.append(current_idx)\n",
    "\n",
    "    # Compare with next 5â€“10 rows only (assumes sorted by title)\n",
    "    for j in range(i + 1, min(i + 15, len(df))):\n",
    "        next_title = df.loc[j, \"job_key\"]\n",
    "        next_skills = set(df.loc[j, \"normalized_skills\"])\n",
    "\n",
    "        # If job title is same\n",
    "        if current_title == next_title:\n",
    "            jaccard_sim = len(current_skills & next_skills) / len(current_skills | next_skills)\n",
    "            if jaccard_sim > 0.85:  # Threshold for \"very similar\" skills\n",
    "                seen.add(j)  # Mark as duplicate\n",
    "\n",
    "# Final cleaned DataFrame\n",
    "cleaned_df = df.loc[keep_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"Reduced from {len(df)} rows to {len(cleaned_df)} rows.\")\n",
    "\n",
    "# Save cleaned data\n",
    "cleaned_df.to_csv(\"final_jobs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f734be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_jobs.csv\")\n",
    "\n",
    "df[\"job_text\"] = df[\"job_title\"].fillna(\"\") + \" - \" + df[\"skill_text\"].fillna(\"\")\n",
    "\n",
    "print(\"Encoding job postings...\")\n",
    "job_embeddings = model.encode(df[\"job_text\"].tolist(), show_progress_bar=True, batch_size=128)\n",
    "\n",
    "embedding_dim = job_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(job_embeddings)\n",
    "\n",
    "faiss.write_index(index, \"faiss_jobs_cleaned.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd806696",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = model.encode(df[\"job_title\"].astype(str).tolist(), show_progress_bar=True, batch_size=128)\n",
    "\n",
    "title_index = faiss.IndexFlatL2(title_embeddings.shape[1])\n",
    "title_index.add(title_embeddings)\n",
    "\n",
    "faiss.write_index(title_index, \"faiss_job_titles.index\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
